---
title: "Homework 5"
author: "Erfan Faridmoayer"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2

This includes data from various locations of crime, with names of victims, date of crime, victim demographics, and their final disposition. 

```{r}
homi_df = read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names()

homi_tidy = homi_df %>% 
  mutate(city_state = str_c(city, ", ", state)) %>%
  group_by(city_state) %>% 
  summarize(
    disposition_n = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
    ) %>% 
  ungroup() %>% 
  select(city_state, disposition_n, unsolved)
```

The dataset `hamid_df` is `r nrow(homi_df)` and `r ncol(homi_df)` in length. After tidying the dataset, I created a `city_state` vector using the `str_c` function to add the city and state columns. Fixed the issue of Tulsa being assigned to the wrong state. The `sum()` function was then used as a logical vector in `summarize()` to showcase the total number of homicides and the number of unsolved ones.

```{r}
balt_df =
  homi_tidy %>% 
  filter(city_state == "Baltimore, MD") %>% 
  mutate(p_obj = map2(unsolved, disposition_n, ~ prop.test(.x, .y) %>%
    broom::tidy())
    ) %>%
    unnest() %>% 
  select(city_state, estimate, "ci_lower" = conf.low, "ci_upper" = conf.high)
```

In the above code, city of Baltimore, MD information was filtered and `prop.test` was used to estimate the proportion of homicides that are unsolved. `purrr::map2` was used to utilize unsolved and total crime cases. The result was saved in `p_obj` object. Lastly, the estimated portion and the confidence intervals were selected.

```{r}
city_list = list(homi_tidy$city_state)

for (i in 1:51) {
  
df = function(z) {
  
  homi_tidy %>% 
  filter(city_state == "city_list[[z]]") %>% 
  mutate(p_obj = map2(unsolved, disposition_n, ~ prop.test(.x, .y) %>%
    broom::tidy())
    ) %>%
    unnest() %>% 
  select(city_state, estimate, "ci_lower" = conf.low, "ci_upper" = conf.high)
    
}
}
```

In the code chunk above, I attempted to create a list to feed into a function as a loop to apply the same code I worked on earlier to all the 51 cities. 


## Problem 3

```{r}

sim_mean_sd = function(n=30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
  
  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
}
```

Above we have described a simulation setting the parameters of fixed n=30, mu=0, sigma=5. Below we will run a simulation to generate 5000 datasets after setting mu=0. 

```{r}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_mean_sd(mu = 0)
}

sim_results = bind_rows(output)
```

below we take a stab at the t-test for the 5000 smis, and showcase the mu_hat, sigma_hat, estimate, and p-value
```{r}
t_tst = 
  sim_results %>%
  mutate(
    t_test_res = map(.x = output, ~t.test(.x) %>% broom::tidy())
  ) %>% 
  unnest() %>%
  select(mu_hat, sigma_hat, estimate, "p_value" = p.value)
```


